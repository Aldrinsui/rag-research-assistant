# ğŸ¤– Multi-Agent RAG Research Assistant

A sophisticated AI research system leveraging LangGraph for autonomous agent orchestration and Retrieval-Augmented Generation (RAG) for context-aware responses.

## ğŸ¯ Project Overview

This project implements an agentic AI system with autonomous multi-agent workflows using state graph orchestration. The system uses RAG (Retrieval-Augmented Generation) to ground responses in a knowledge base, reducing hallucinations and improving accuracy.

### Key Features

## ğŸ”¬ Technical Design Decisions

### Vector Database Selection
**Chose ChromaDB** over Pinecone/Weaviate for this implementation:
- âœ… Local-first development (no external API dependencies)
- âœ… Native LangChain integration
- âœ… Fastest path to working prototype

**Production Considerations:**
- **Pinecone**: Managed service, better for scale (millions of vectors)
- **Weaviate**: Superior hybrid search, advanced filtering capabilities
- **ChromaDB**: Excellent for prototyping, self-hosted production

### Evaluation & Quality Metrics
Current implementation tracks:
- Source attribution accuracy (% of responses with citations)
- Retrieval relevance (semantic search precision)
- Response latency

**Next Steps for Production:**
- RAGAS metrics: Faithfulness, answer relevance, context precision
- DeepEval: Hallucination detection, toxicity checks
- Custom test dataset with ground truth Q&A pairs

### Agent Architecture Philosophy
**Sequential workflow** (Retrieve â†’ Research â†’ Synthesize):
- âœ… Clear separation of concerns
- âœ… Easy to debug and monitor
- âœ… Predictable behavior

**Considered but deferred:**
- Self-reflection loops (validator agent checking synthesis quality)
- Multi-path reasoning with agent voting
- Dynamic replanning based on retrieval quality
- 
## ğŸ—ï¸ Architecture
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    User Query                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              LangGraph State Machine                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  1. Retrieval Node                                â”‚  â”‚
â”‚  â”‚     - Query embedding                             â”‚  â”‚
â”‚  â”‚     - Vector similarity search (ChromaDB)         â”‚  â”‚
â”‚  â”‚     - Context extraction                          â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                  â”‚                                       â”‚
â”‚                  â–¼                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  2. Research Agent                                â”‚  â”‚
â”‚  â”‚     - Context analysis                            â”‚  â”‚
â”‚  â”‚     - Information extraction                      â”‚  â”‚
â”‚  â”‚     - Source attribution                          â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                  â”‚                                       â”‚
â”‚                  â–¼                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  3. Synthesizer Agent                             â”‚  â”‚
â”‚  â”‚     - Response generation                         â”‚  â”‚
â”‚  â”‚     - Citation formatting                         â”‚  â”‚
â”‚  â”‚     - Quality assurance                           â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
           Final Answer with Citations
```

## ğŸ› ï¸ Tech Stack

- **LangGraph**: State graph orchestration for agent workflows
- **LangChain**: Framework for LLM application development
- **Hugging Face**: LLM (Flan-T5) and embeddings (MiniLM)
- **ChromaDB**: Vector database for semantic search
- **Streamlit**: Interactive web interface
- **Python 3.11+**: Core programming language

## ğŸ“¦ Installation

### Prerequisites
- Python 3.11 or higher
- Hugging Face API key (free)

### Setup

1. **Clone the repository**
```bash
git clone <your-repo-url>
cd rag-research-assistant
```

2. **Create virtual environment**
```bash
python3 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

3. **Install dependencies**
```bash
pip install -r requirements.txt
```

4. **Configure environment variables**
```bash
cp .env.example .env
# Edit .env and add your Hugging Face API key
```

## ğŸš€ Usage

### Run the Streamlit App
```bash
streamlit run app.py
```

The app will open in your browser at `http://localhost:8501`

### Test the System
```bash
python test_system.py
```

### Example Queries
- "What is machine learning?"
- "Explain transformer architecture"
- "How does RAG work?"
- "What are autonomous agents?"

## ğŸ“Š Performance Metrics

- **Retrieval Accuracy**: Semantic search across knowledge base
- **Response Time**: ~1-2 seconds per query
- **Source Attribution**: 100% of responses cite original documents
- **Agent Steps**: 3-step workflow (Retrieve â†’ Research â†’ Synthesize)

## ğŸ§ª Project Structure
```
rag-research-assistant/
â”œâ”€â”€ agents/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ graph_agents.py          # LangGraph agent implementation
â”œâ”€â”€ rag/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ embeddings.py            # Document processing & embeddings
â”‚   â”œâ”€â”€ vectordb.py              # ChromaDB vector store
â”‚   â””â”€â”€ retrieval.py             # RAG retrieval logic
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ config.py                # Configuration management
â”œâ”€â”€ data/
â”‚   â””â”€â”€ documents/               # Knowledge base documents
â”œâ”€â”€ app.py                       # Streamlit UI
â”œâ”€â”€ test_system.py               # Testing script
â”œâ”€â”€ create_sample_docs.py        # Generate sample data
â”œâ”€â”€ requirements.txt             # Python dependencies
â”œâ”€â”€ .env                         # Environment variables
â””â”€â”€ README.md                    # This file
```

## ğŸ“ Key Concepts Demonstrated

### 1. Agentic AI
- Autonomous decision-making agents
- Multi-step reasoning workflows
- Tool use and context management

### 2. RAG (Retrieval-Augmented Generation)
- Vector embeddings for semantic search
- Context injection into LLM prompts
- Grounded responses with source attribution

### 3. State Graph Orchestration
- LangGraph state machines
- Node-based agent workflows
- Sequential and parallel processing

### 4. Production-Ready Patterns
- Error handling and fallbacks
- Performance monitoring
- Modular architecture

## ğŸ”§ Customization

### Add Your Own Documents
Place `.txt` files in `data/documents/` and restart the system.

### Change the LLM Model
Edit `.env` and update `MODEL_NAME` to any Hugging Face model:
```
MODEL_NAME=google/flan-t5-large
```

### Adjust RAG Parameters
In `utils/config.py`:
```python
CHUNK_SIZE = 1000        # Document chunk size
CHUNK_OVERLAP = 200      # Overlap between chunks
```

## ğŸ“ˆ Future Enhancements

- [ ] Add web search capability for real-time information
- [ ] Implement conversation memory
- [ ] Add multi-document comparison
- [ ] Deploy as API service
- [ ] Add evaluation metrics dashboard

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## ğŸ“„ License

This project is licensed under the MIT License.

## ğŸ‘¤ Author

**Your Name**
- GitHub: [@aldrinsui](https://github.com/aldrinsui)
- Email: aldrinjerry24@gmail.com

## ğŸ™ Acknowledgments

- Built for AI/ML Internship application at Stackular
- Inspired by modern agentic AI frameworks (CrewAI, AutoGen)
- Uses open-source tools from LangChain, Hugging Face, and Streamlit communities

---

**â­ If you find this project interesting, please consider starring it!**
